---
navigation_title: "Run Kibana in production"
mapped_pages:
  - https://www.elastic.co/guide/en/kibana/current/production.html
applies_to:
  deployment:
    ess: all
    ece: all
    eck: all
    self: all
---

# Use Kibana in production environments [production]

How you deploy {{kib}} largely depends on your use case. If you are the only user, you can run {{kib}} on your local machine and configure it to point to whatever {{es}} instance you want to interact with. Conversely, if you have a large number of heavy {{kib}} users, you might need to load balance across multiple {{kib}} instances that are all connected to the same {{es}} cluster or deployment.

Historically, Kibana’s scalability was primarily influenced by the number of concurrent users and the complexity of dashboards and visualizations. However, with the introduction of new capabilities such as [Kibana Alerting](/explore-analyze/alerts-cases.md) and the [Detection Rules](/solutions/security/detect-and-alert.md) engine, critical components for [Observability](/solutions/observability.md) and [Security](/solutions/security.md) solutions, the scalability factors have evolved significantly.

Now, Kibana’s resource requirements extend beyond user activity. The system must also handle workloads generated by automated processes, such as scheduled alerts, background detection rules, and other periodic tasks. These operations are managed by [Kibana Task Manager](./kibana-task-manager-scaling-considerations.md), which is responsible for scheduling, executing, and coordinating all background tasks.

Additionally, the task manager enables distributed coordination across multiple Kibana instances, allowing Kibana to function as a logical cluster in certain aspects.

::::{important} 
* {{kib}} does not support rolling [upgrades](/deploy-manage/upgrade/deployment-or-cluster/kibana.md), and deploying mixed versions of {{kib}} can result in data loss or upgrade failures. Please shut down all instances of {{kib}} before performing an upgrade, and ensure all running {{kib}} instances have matching versions.
* While {{kib}} isn’t resource intensive, we still recommend running {{kib}} separate from  your {{es}} data or master nodes.
::::

## Section overview

This section provides guidance on key configurations and optimizations for running {{kib}} in production environments. You’ll learn how to scale, secure, and optimize {{kib}} for high availability and performance, as well as how to manage background tasks and other features effectively.

::::{note}
Note for deployment types, as not all the sub-sections are applicable to all deployment types.
::::

Topics covered in this section:

* [Load balancing across multiple {{kib}} instances within the same deployment](#load-balancing-kibana)
* [Accessing multiple load-balanced {{kib}} deployments](#accessing-load-balanced-kibana)
* [Distributing {{kib}} traffic across multiple {{es}} instances](#high-availability)
* [Setting a memory limit to {{kib}}](#memory)
* [Disabling legacy encryption algorithms for OpenSSL](#openssl-legacy-provider)
* [Kibana task manager: Scaling and performance](./kibana-task-manager-scaling-considerations.md)
* [Kibana alerting: Scaling and production considerations](./kibana-alerting-production-considerations.md)
* [Kibana reporting production considerations](./kibana-reporting-production-considerations.md)

Other sections of the documentation also includes important topics:

* [](/deploy-manage/security/secure-your-cluster-deployment.md)

## Load balancing across multiple {{kib}} instances [load-balancing-kibana]
```yaml {applies_to}
deployment:
  self: all
```

To run multiple {{kib}} instances connected to the same {{es}} cluster, you need to adjust the configuration. See the [{{kib}} configuration reference](kibana://reference/configuration-reference) for details on each setting.

::::{note}
When adding multiple {{kib}} instances to the same deployment in {{ech}}, {{ece}}, or {{eck}}, the orchestrator applies the necessary configuration, requiring no manual setup.
::::

* These settings must be **unique** across each {{kib}} instance:

  ```js
  server.uuid // if not provided, this is autogenerated
  server.name
  path.data
  pid.file
  server.port
  ```

* When using a file appender, the target file must also be unique:

  ```yaml
  logging:
    appenders:
      default:
        type: file
        fileName: /unique/path/per/instance
  ```

* These settings must be **the same** for all {{kib}} belonging to the same cluster or deployment:

  ```js
  xpack.security.encryptionKey // decrypting session information
  xpack.security.authc.* // authentication configuration
  xpack.security.session.* // session configuration
  xpack.reporting.encryptionKey // decrypting reports
  xpack.encryptedSavedObjects.encryptionKey // decrypting saved objects
  xpack.encryptedSavedObjects.keyRotation.decryptionOnlyKeys // saved objects encryption key rotation, if any
  ```

  ::::{warning} 
  If the authentication configuration does not match, sessions from unrecognized providers in each {{kib}} instance will be deleted during that instance’s regular session cleanup. Similarly, inconsistencies in session configuration can also lead to undesired session logouts. This also applies to any {{kib}} instances that are backed by the same {{es}} instance and share the same kibana.index, even if they are not behind the same load balancer.
  ::::

* Separate configuration files can be used from the command line by using the `-c` flag:

  ```js
  bin/kibana -c config/instance1.yml
  bin/kibana -c config/instance2.yml
  ```

## Accessing multiple load-balanced {{kib}} deployments [accessing-load-balanced-kibana] 
```yaml {applies_to}
deployment:
  self: all
```

To access multiple load-balanced {{kib}} deployments from the same browser, explicitly set `xpack.security.cookieName` to the same value across all {{kib}} instances within the same cluster, and use different values for other clusters.

This prevents cookie conflicts between {{kib}} instances, ensuring seamless high availability and maintaining the session active in case of an instance failure.

::::{note}
In this context, a Kibana cluster or deployment refers to multiple Kibana instances connected to the same {{es}} cluster.
::::

## High availability across multiple {{es}} nodes [high-availability]
```yaml {applies_to}
deployment:
  self: all
```

{{kib}} can be configured to connect to multiple {{es}} nodes in the same cluster.  In situations where a node becomes unavailable, {{kib}} will transparently connect to an available node and continue operating.  Requests to available hosts will be routed in a round robin fashion (except for Dev Tools which will connect only to the first node available).

In kibana.yml:

```js
elasticsearch.hosts:
  - http://elasticsearch1:9200
  - http://elasticsearch2:9200
```

Related configurations include `elasticsearch.sniffInterval`, `elasticsearch.sniffOnStart`, and `elasticsearch.sniffOnConnectionFault`. These can be used to automatically update the list of hosts as a cluster is resized.  Parameters can be found in the [{{kib}} configuration reference](kibana://reference/configuration-reference/general-settings.md).

::::{note}
The previous configuration can be useful when there is no load balancer or reverse proxy in front of {{es}}. If a load balancer is in place to distribute traffic among {{es}} instances, Kibana should be configured to connect to it instead. 

In [orchestrated deployments](/deploy-manage/deploy.md#about-orchestration), {{kib}} is automatically configured to connect to {{es}} through a load-balanced service.
::::

## Memory [memory] 

Kibana has a default memory limit that scales based on total memory available.  In some scenarios, such as large reporting jobs, it may make sense to tweak limits to meet more specific requirements.

A limit can be defined by setting `--max-old-space-size` in the `node.options` config file found inside the `kibana/config` folder or any other folder configured with the environment variable `KBN_PATH_CONF`. For example, in the Debian-based system, the folder is `/etc/kibana`.

The option accepts a limit in MB:

```js
--max-old-space-size=2048
```

## OpenSSL Legacy Provider [openssl-legacy-provider] 

Starting in 8.10.0, {{kib}} has upgraded its runtime environment, Node.js, from version 16 to version 18 and with it the underlying version of OpenSSL to version 3. Algorithms deemed legacy by OpenSSL 3 have been re-enabled to avoid potential breaking changes in a minor version release of {{kib}}. If SSL certificates configured for {{kib}} are not using any of the legacy algorithms mentioned in the [OpenSSL legacy provider documentation](https://www.openssl.org/docs/man3.0/man7/OSSL_PROVIDER-legacy.md), we recommend disabling this setting by removing `--openssl-legacy-provider` in the `node.options` config file.

