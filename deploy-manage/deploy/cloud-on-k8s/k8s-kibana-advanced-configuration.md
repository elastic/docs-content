---
applies:
  eck: all
mapped_pages:
  - https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-kibana-advanced-configuration.html
---

# Advanced configuration [k8s-kibana-advanced-configuration]

If you already looked at the [Elasticsearch on ECK](elasticsearch-configuration.md) documentation, some of these concepts might sound familiar to you. The resource definitions in ECK share the same philosophy when you want to:

* [Customize the Pod configuration](#k8s-kibana-pod-configuration)
* [Customize the product configuration](#k8s-kibana-configuration)
* [Manage HTTP settings](k8s-kibana-http-configuration.md)
* [Use secure settings](k8s-kibana-secure-settings.md)
* [Install {{kib}} plugins](k8s-kibana-plugins.md)

## Pod configuration [k8s-kibana-pod-configuration]

You can [customize the {{kib}} Pod](customize-pods.md) using a [Pod Template](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates).

The following example demonstrates how to create a {{kib}} deployment with custom node affinity, increased heap size, and resource limits.

```yaml
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana-sample
spec:
  version: 8.16.1
  count: 1
  elasticsearchRef:
    name: "elasticsearch-sample"
  podTemplate:
    spec:
      containers:
      - name: kibana
        env:
          - name: NODE_OPTIONS
            value: "--max-old-space-size=2048"
        resources:
          requests:
            memory: 1Gi
            cpu: 0.5
          limits:
            memory: 2.5Gi
            cpu: 2
      nodeSelector:
        type: frontend
```

The name of the container in the [Pod Template](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates) must be `kibana`.

Check [Set compute resources for Kibana, Elastic Maps Server, APM Server and Logstash](manage-compute-resources.md#k8s-compute-resources-kibana-and-apm) for more information.


## {{kib}} configuration [k8s-kibana-configuration]

You can add your own {{kib}} settings to the `spec.config` section.

The following example demonstrates how to set the [`elasticsearch.requestHeadersWhitelist`](asciidocalypse://docs/kibana/docs/reference/configuration-reference/general-settings.md#elasticsearch-requestHeadersWhitelist) configuration option.

```yaml
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana-sample
spec:
  version: 8.16.1
  count: 1
  elasticsearchRef:
    name: "elasticsearch-sample"
  config:
     elasticsearch.requestHeadersWhitelist:
     - authorization
```


## Scale out a {{kib}} deployment [k8s-kibana-scaling]

To deploy more than one instance of {{kib}}, the instances must share a matching set of encryption keys. The following keys are automatically generated by the operator:

* [`xpack.security.encryptionKey`](asciidocalypse://docs/kibana/docs/reference/configuration-reference/security-settings.md#xpack-security-encryptionKey)
* [`xpack.reporting.encryptionKey`](asciidocalypse://docs/kibana/docs/reference/configuration-reference/reporting-settings.md#encryption-keys)
* [`xpack.encryptedSavedObjects.encryptionKey`](/deploy-manage/security/secure-saved-objects.md)

::::{tip}
If you need to access these encryption keys, you can find them using the `kubectl get secrets` command.

The secret is named after the corresponding Kibana instance. For example, for a Kibana named `my-kibana`, you can run the following command to retrieve the current encryption keys:

```shell
kubectl get secret my-kibana-kb-config -o jsonpath='{ .data.kibana\.yml }' | base64 --decode | grep -A1 encryptedSavedObjects
```

::::


You can provide your own encryption keys using a secure setting, as described in [Secure settings](k8s-kibana-secure-settings.md).

::::{note}
While most reconfigurations of your {{kib}} instances are carried out in rolling upgrade fashion, all version upgrades will cause {{kib}} downtime. This happens because you can only run a single version of {{kib}} at any given time. For more information, check [Upgrade {{kib}}](/deploy-manage/upgrade/deployment-or-cluster.md).
::::



