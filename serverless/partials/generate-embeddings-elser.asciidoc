This is how an ingest pipeline that uses the ELSER model is created:

[source,bash]
----
curl -X PUT "${ES_URL}/_ingest/pipeline/my-text-embeddings-pipeline" \
-H "Authorization: ApiKey ${API_KEY}" \
-H "Content-Type: application/json" \
-d'
{
  "description": "Text embedding pipeline",
  "processors": [
    {
      "inference": {
        "model_id": ".elser_model_2",
        "input_output": [
          {
            "input_field": "my_text_field",
            "output_field": "my_tokens"
          }
        ]
      }
    }
  ]
}
'
----

To ingest data through the pipeline to generate tokens with ELSER, refer to the
<<reindexing-data-elser,Ingest the data through the {infer} ingest pipeline>> section of the tutorial. After you successfully
ingested documents by using the pipeline, your index will contain the tokens
generated by ELSER.
