---
# slug: /serverless/elasticsearch/explore-your-data-ml-nlp/inference
title: Add NLP ((infer)) to ingest pipelines
description: You can import trained models into your cluster and configure them for specific NLP tasks.
tags: [ 'serverless', 'elasticsearch', 'tbd' ]
---

<DocBadge template="technical preview" />
After you <DocLink slug="/serverless/elasticsearch/explore-your-data-ml-nlp/deploy-trained-models">deploy a trained model in your cluster</DocLink>, 
you can use it to perform ((nlp)) tasks in ingest pipelines.

1. Verify that all of the
    [ingest pipeline prerequisites](((ref))/ingest.html#ingest-prerequisites) 
    are met.

1. <DocLink slug="/serverless/elasticsearch/explore-your-data-ml-nlp/inference" section="add-an-((infer))-processor-to-an-ingest-pipeline">Add an ((infer)) processor to an ingest pipeline</DocLink>.
1. <DocLink slug="/serverless/elasticsearch/explore-your-data-ml-nlp/inference" section="ingest-documents">Ingest documents</DocLink>.
1. <DocLink slug="/serverless/elasticsearch/explore-your-data-ml-nlp/inference" section="view-the-results">View the results</DocLink>.


## Add an ((infer)) processor to an ingest pipeline

In ((kib)), you can create and edit pipelines under **Content** → **Pipelines**.

<div style={{ textAlign: "center" }}><DocImage url="../images/ml-nlp-pipeline-lang.png" alt="Creating a pipeline in the Management app" /></div>

1. Click **Create pipeline** or edit an existing pipeline.
1. Add an [((infer)) processor](((ref))/inference-processor.html) to your pipeline:
    1. Click **Add a processor** and select the **((infer-cap))** processor type.
    1. Set **Model ID** to the name of your trained model, for example
        `elastic__distilbert-base-cased-finetuned-conll03-english` or
        `lang_ident_model_1`.

    1. If you use the ((lang-ident)) model (`lang_ident_model_1`) that is provided in
        your cluster:

        1. The input field name is assumed to be `text`. If you want to identify
            languages in a field with a different name, you must map your field name to
            `text` in the **Field map** section. For example:

            ```js
            {
            "message": "text"
            }
            ```
            {/* NOTCONSOLE */}

    1. Click **Add** to save the processor.
1. Optional: Add a [set processor](((ref))/set-processor.html) to index the ingest
    timestamp.

    1. Click **Add a processor** and select the **Set** processor type.
    1. Choose a name for the field (such as `event.ingested`) and set its value to
        `{{{_ingest.timestamp}}}`. For more details, refer to
        [Access ingest metadata in a processor](((ref))/ingest.html#access-ingest-metadata).

    1. Click **Add** to save the processor.
1. Optional: Add failure processors to handle exceptions. For example, in the
    **Failure processors** section:

    1. Add a set processor to capture the
        pipeline error message. Choose a name for the field (such as
        `ml.inference_failure`) and set its value to the
        `{{_ingest.on_failure_message}}` document metadata field.

    1. Add a set processor to reroute
        problematic documents to a different index for troubleshooting purposes. Use
        the `_index` metadata field and set its value to a new name (such as
        `failed-{{{ _index }}}`). For more details, refer
        to [Handling pipeline failures](((ref))/ingest.html#handling-pipeline-failures).

1. To test the pipeline, click **Add documents**.
    1. In the **Documents** tab, provide a sample document for testing.

        For example, to test a trained model that performs named entity recognition
        (NER):

        ```js
        [
        {
        "_source": {
        "text_field":"Hello, my name is Josh and I live in Berlin."
        }
        }
        ]
        ```
        {/* NOTCONSOLE */}

        To test a trained model that performs ((lang-ident)):

        ```js
        [
        {
        "_source":{
        "message":"Sziasztok! Ez egy rövid magyar szöveg. Nézzük, vajon sikerül-e azonosítania a language identification funkciónak? Annak ellenére is sikerülni fog, hogy a szöveg két angol szót is tartalmaz."
        }
        }
        ]
        ```
        {/* NOTCONSOLE */}

    1. Click **Run the pipeline** and verify the pipeline worked as expected.

        In the ((lang-ident)) example, the predicted value is the ISO identifier of the
        language with the highest probability. In this case, it should be `hu` for
        Hungarian.

    1. If everything looks correct, close the panel, and click **Create
        pipeline**. The pipeline is now ready for use.

{/*

<DocAccordion buttonContent="API example">

```console
POST _ingest/pipeline/my-ner-pipeline
{
  "inference": {
    "model_id": "elastic__distilbert-base-cased-finetuned-conll03-english",
    "field_map": {
      "review": "text_field"
    },
    "on_failure": [
      {
        "set": {
          "description": "Set the error message",
          "field": "ml.inference_failure",
          "value": "{{_ingest.on_failure_message}}"
        }
      },
      {
        "set": {
          "description": "Index document to 'failed-<index>'",
          "field": "_index",
          "value": "failed-{{{ _index }}}"
        }
      }
    ]
  }
}
```
TEST[skip:TBD]

</DocAccordion>

*/}

## Ingest documents

You can now use your ingest pipeline to perform NLP tasks on your data.

Before you add data, consider which mappings you want to use. For example, you
can create explicit mappings with the create index API in the
**((dev-tools-app))** → **Console**:

```console
PUT ner-test
{
  "mappings": {
    "properties": {
      "ml.inference.predicted_value": {"type": "annotated_text"},
      "ml.inference.model_id": {"type": "keyword"},
      "text_field": {"type": "text"},
      "event.ingested": {"type": "date"}
    }
  }
}
```
{/* TEST[skip:TBD] */}

<DocCallOut title="Tip">
To use the `annotated_text` data type in this example, you must install the
[mapper annotated text plugin](((plugins))/mapper-annotated-text.html). For more
installation details, refer to   
[Add plugins provided with ((ess))](((cloud))/ec-adding-elastic-plugins.html).
</DocCallOut>

You can then use the new pipeline to index some documents. For example, use a
bulk indexing request with the `pipeline` query parameter for your NER pipeline:

```console
POST /_bulk?pipeline=my-ner-pipeline
{"create":{"_index":"ner-test","_id":"1"}}
{"text_field":"Hello, my name is Josh and I live in Berlin."}
{"create":{"_index":"ner-test","_id":"2"}}
{"text_field":"I work for Elastic which was founded in Amsterdam."}
{"create":{"_index":"ner-test","_id":"3"}}
{"text_field":"Elastic has headquarters in Mountain View, California."}
{"create":{"_index":"ner-test","_id":"4"}}
{"text_field":"Elastic's founder, Shay Banon, created Elasticsearch to solve a simple need: finding recipes!"}
{"create":{"_index":"ner-test","_id":"5"}}
{"text_field":"Elasticsearch is built using Lucene, an open source search library."}
```
{/* TEST[skip:TBD] */}

Or use an individual indexing request with the `pipeline` query parameter for
your ((lang-ident)) pipeline:

```console
POST lang-test/_doc?pipeline=my-lang-pipeline
{
  "message": "Mon pays ce n'est pas un pays, c'est l'hiver"
}
```
{/* TEST[skip:TBD] */}

You can also use NLP pipelines when you are reindexing documents to a new
destination. For example, since the
[sample web logs data set](((kibana-ref))/get-started.html#gs-get-data-into-kibana)
contain a `message` text field, you can reindex it with your ((lang-ident))
pipeline:

```console
POST _reindex
{
  "source": {
    "index": "kibana_sample_data_logs",
    "size": 50
  },
  "dest": {
    "index": "lang-test",
    "pipeline": "my-lang-pipeline"
  }
}
```
{/* TEST[skip:TBD] */}

However, those web log messages are unlikely to contain enough words for the
model to accurately identify the language.

<DocCallOut title="Tip">
Set the reindex `size` option to a value smaller than the `queue_capacity` 
for the trained model deployment. Otherwise, requests might be rejected with a 
"too many requests" 429 error code.
</DocCallOut>


## View the results

Before you can verify the results of the pipelines, you must
[create ((data-sources))](((kibana-ref))/data-views.html). Then you can explore 
your data in **Discover**:

<div style={{ textAlign: "center" }}><DocImage url="../images/ml-nlp-discover-ner.png" alt="A document from the NER pipeline in the Discover app" /></div>

The `ml.inference.predicted_value` field contains the output from the ((infer))
processor. In this NER example, there are two documents that contain the
`Elastic` organization entity.

In this ((lang-ident)) example, the `ml.inference.predicted_value` contains the 	
ISO identifier of the language with the highest probability and the
`ml.inference.top_classes` fields contain the top five most probable languages
and their scores:

<div style={{ textAlign: "center" }}><DocImage url="../images/ml-nlp-discover-lang.png" alt="A document from the ((lang-ident)) pipeline in the Discover app" /></div>

To learn more about ingest pipelines and all of the other processors that you
can add, refer to [Ingest pipelines](((ref))/ingest.html).


## Common problems

If you encounter problems while using your trained model in an ingest pipeline,
check the following possible causes:

1. The trained model is not deployed in your cluster. You can view its status in
    **((ml-app))** → **Model Management** or use the
    [get trained models statistics API](((ref))/get-trained-models-stats.html). 
    Unless you are using the built-in `lang_ident_model_1` model, you must 
    ensure your model is successfully deployed. Refer to 
    <DocLink slug="/serverless/elasticsearch/explore-your-data-ml-nlp/deploy-trained-models">Deploy the model in your cluster</DocLink>.

1. The default input field name expected by your trained model is not present in
    your source document. Use the **Field Map** option in your ((infer)) 
    processor to set the appropriate field name.

1. There are too many requests. If you are using bulk ingest, reduce the number
    of documents in the bulk request. If you are reindexing, use the `size`
    parameter to decrease the number of documents processed in each batch.

These common failure scenarios and others can be captured by adding failure
processors to your pipeline. For more examples, refer to
[Handling pipeline failures](((ref))/ingest.html#handling-pipeline-failures).


## Further reading

* [How to deploy NLP: Text Embeddings and Vector Search](((blog-ref))how-to-deploy-nlp-text-embeddings-and-vector-search)
* [How to deploy NLP: Named entity recognition (NER) example](((blog-ref))how-to-deploy-nlp-named-entity-recognition-ner-example)
* [How to deploy NLP: Sentiment Analysis Example](((blog-ref))how-to-deploy-nlp-sentiment-analysis-example)